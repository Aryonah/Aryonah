{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtXTHDEZBwZH"
      },
      "source": [
        "# Pertemuan 5 - Validasi Kualitas dan Hasil Analisis Dataset\n",
        "\n",
        "## Tujuan Pembelajaran (Sub-CPMK 2.1):\n",
        "> Mampu memvalidasi kualitas dan hasil analisis dataset dengan pendekatan seperti k-fold cross-validation dan bootstrap sampling untuk memastikan dan meningkatkan ketepatan dan keandalan model.\n",
        "\n",
        "## Pokok Bahasan\n",
        "1. Data Cleaning\n",
        "2. Data Normalization\n",
        "3. Validasi Model\n",
        "4. K-Fold Cross-Validation\n",
        "5. Confussion Matrix\n",
        "6. Bootstrap Sampling\n",
        "7. Evaluasi Kinerja Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SBHQ9hZBwZI"
      },
      "source": [
        "## 1. Data Cleaning\n",
        "Data cleaning adalah proses penting dalam analisis data yang bertujuan untuk memastikan data yang digunakan bersih, akurat, dan siap untuk analisis lebih lanjut. Proses ini mencakup beberapa langkah dan teknik yang dirancang untuk menangani berbagai masalah data, seperti missing values, duplikat, outliers, dan kesalahan format. Berikut adalah langkah-langkah dan teknik umum dalam data cleaning:\n",
        "![1_CMsrcN6xYIVv4SXISqikbA.jpg](attachment:1_CMsrcN6xYIVv4SXISqikbA.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CbZ1hcEBwZJ"
      },
      "source": [
        "### A. Mengatasi Missing Values\n",
        "Missing values atau nilai yang hilang dalam dataset merupakan masalah yang sering dihadapi dalam proses machine learning (ML). Menangani missing values sangat penting karena beberapa alasan yang krusial bagi kualitas dan akurasi model. Pertama, data yang hilang dapat mengurangi kualitas keseluruhan dataset. Model ML bergantung pada data untuk mempelajari pola dan membuat prediksi. Jika data yang digunakan tidak lengkap, model mungkin tidak dapat menangkap pola yang sebenarnya, yang pada gilirannya dapat menghasilkan prediksi yang tidak akurat dan tidak dapat diandalkan. Dalam banyak kasus, data yang hilang bisa menyebabkan model tidak bekerja dengan baik atau bahkan gagal berfungsi. Banyak algoritma ML, seperti regresi linier atau support vector machine (SVM), tidak dapat menangani missing values secara langsung. Algoritma ini memerlukan data yang lengkap untuk melakukan perhitungan dan analisis. Jika ada nilai yang hilang dalam dataset, algoritma ini bisa memberikan hasil yang salah atau bahkan tidak dapat beroperasi sama sekali.\n",
        "\n",
        "Nilai yang hilang juga dapat menyebabkan bias dalam model jika tidak ditangani dengan benar. Misalnya, jika missing values tidak terdistribusi secara acak, ini bisa mengakibatkan model belajar dari subset data yang tidak representatif dari populasi sebenarnya. Hal ini dapat menyebabkan kesimpulan yang bias dan menyesatkan, mengurangi keandalan model dalam dunia nyata. Selain itu, menghapus baris atau kolom dengan missing values bisa mengurangi ukuran dataset secara signifikan, terutama jika jumlah nilai yang hilang banyak. Penghapusan ini dapat mengurangi kekayaan informasi dalam dataset, menghambat kemampuan model untuk belajar secara efektif dari data yang tersedia.\n",
        "\n",
        "Penanganan missing values dapat dilakukan melalui beberapa metode, seperti penghapusan baris atau kolom yang tidak lengkap jika jumlahnya sedikit, atau menggunakan teknik imputasi untuk mengisi nilai yang hilang dengan rata-rata, median, atau metode prediktif lainnya. Ada juga beberapa algoritma seperti Random Forest dan XGBoost yang dapat menangani missing values secara internal. Dengan menangani missing values dengan baik, kita dapat memastikan bahwa data yang digunakan adalah representatif dan berkualitas, yang pada akhirnya akan menghasilkan model ML yang lebih akurat dan andal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "j-4QcX0pBwZK",
        "outputId": "a4d6e18b-c30c-4225-8266-e464126779d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Awal:\n",
            "      Name   Age         City\n",
            "0    Alice  25.0     New York\n",
            "1      Bob  30.0  Los Angeles\n",
            "2  Charlie  35.0      Chicago\n",
            "3    Alice  25.0     New York\n",
            "4      Eve   NaN        Miami\n",
            "5      NaN  50.0  Los Angeles\n",
            "\n",
            "Data Setelah Cleaning:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Name   Age         City\n",
              "0    Alice  25.0     New York\n",
              "1      Bob  30.0  Los Angeles\n",
              "2  Charlie  35.0      Chicago\n",
              "4      Eve  32.5        Miami"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6a99557-d9f6-42bb-993e-5a10bfbe12ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Age</th>\n",
              "      <th>City</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alice</td>\n",
              "      <td>25.0</td>\n",
              "      <td>New York</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bob</td>\n",
              "      <td>30.0</td>\n",
              "      <td>Los Angeles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Charlie</td>\n",
              "      <td>35.0</td>\n",
              "      <td>Chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eve</td>\n",
              "      <td>32.5</td>\n",
              "      <td>Miami</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6a99557-d9f6-42bb-993e-5a10bfbe12ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6a99557-d9f6-42bb-993e-5a10bfbe12ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6a99557-d9f6-42bb-993e-5a10bfbe12ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46781fb1-73ca-4a97-a694-1fa9ba5931d8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46781fb1-73ca-4a97-a694-1fa9ba5931d8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46781fb1-73ca-4a97-a694-1fa9ba5931d8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7ee02f6c-9ad6-44b6-9362-01280d50347d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7ee02f6c-9ad6-44b6-9362-01280d50347d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Bob\",\n          \"Eve\",\n          \"Alice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.269562819149833,\n        \"min\": 25.0,\n        \"max\": 35.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          30.0,\n          32.5,\n          25.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Los Angeles\",\n          \"Miami\",\n          \"New York\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Contoh Data Cleaning dengan Python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Membuat dataframe contoh\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eve', np.nan],\n",
        "    'Age': [25, 30, 35, 25, np.nan, 50],\n",
        "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Miami', 'Los Angeles']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Menampilkan data awal\n",
        "print(\"Data Awal:\")\n",
        "print(df)\n",
        "\n",
        "# Menghapus duplikat\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Menangani missing values dengan mengisi nilai median untuk kolom numerik\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# Menghapus baris yang mengandung missing values di kolom 'Name'\n",
        "df = df.dropna(subset=['Name'])\n",
        "\n",
        "# Menampilkan data setelah cleaning\n",
        "print(\"\\nData Setelah Cleaning:\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JjRfmd9BwZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1d3d5f-328e-42f9-9d81-13c019d4f126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name    1\n",
            "Age     1\n",
            "City    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Identifikasi Missing Values\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjlMaKwDBwZK"
      },
      "outputs": [],
      "source": [
        "# Mengisi Missing Values\n",
        "df['Age'].fillna(df['Age'].mean())  # Mean\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)  # Median\n",
        "df['Age'].fillna(df['Age'].mode()[0], inplace=True)  # Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XkyudGIBwZL"
      },
      "outputs": [],
      "source": [
        "# Forward/Backward Fill\n",
        "df['Age'].fillna(method='ffill', inplace=True)  # Forward fill\n",
        "df['Age'].fillna(method='bfill', inplace=True)  # Backward fill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCSxK2y1BwZM"
      },
      "outputs": [],
      "source": [
        "# Dropping Missing Values\n",
        "df.dropna(subset=['column'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRDpCSMSBwZM"
      },
      "source": [
        "### B. Menghapus Duplikat\n",
        "Menghapus duplikat dalam dataset sangat penting karena duplikat dapat mendistorsi hasil analisis dan model machine learning. Duplikat menyebabkan overrepresentasi beberapa entri, yang dapat mengarah pada bias dalam perhitungan statistik dan model prediktif. Misalnya, jika satu pelanggan muncul beberapa kali dalam data penjualan, total penjualan atau rata-rata pembelian per pelanggan bisa menjadi tidak akurat. Selain itu, data duplikat dapat meningkatkan kompleksitas dan ukuran dataset, yang mengakibatkan penggunaan memori yang lebih tinggi dan waktu pemrosesan yang lebih lama. Dengan menghapus duplikat, kita memastikan bahwa setiap entri unik hanya muncul sekali, yang membantu menjaga integritas data, meningkatkan efisiensi analisis, dan menghasilkan model yang lebih akurat dan dapat diandalkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh_XhRg0BwZN"
      },
      "outputs": [],
      "source": [
        "# Identifikasi duplikat\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# Menghapus duplikat\n",
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5xTjeHyBwZN"
      },
      "source": [
        "### C. Menangani Outliers\n",
        "Outliers adalah data yang menyimpang secara signifikan dari sebagian besar data dalam dataset. Mereka dapat berada jauh dari nilai rata-rata atau median dan mungkin menunjukkan adanya variasi atau kesalahan dalam data. Outliers dapat disebabkan oleh berbagai faktor, seperti kesalahan pengukuran, kesalahan pencatatan, atau kejadian yang jarang terjadi tetapi sah. Mengapa Outliers Penting?\n",
        "1. Pengaruh pada Statistik Deskriptif: Outliers dapat secara signifikan mempengaruhi statistik seperti mean (rata-rata) dan standard deviation (simpangan baku). Mereka dapat menyebabkan hasil statistik yang menyesatkan.\n",
        "2. Pengaruh pada Model Machine Learning: Dalam machine learning, outliers dapat mempengaruhi kinerja model. Misalnya, model regresi linier dapat condong atau menjadi tidak akurat jika terdapat outliers.\n",
        "3. Indikasi Masalah atau Informasi Berharga: Outliers kadang-kadang dapat menunjukkan kesalahan dalam data, tetapi mereka juga bisa memberikan wawasan berharga tentang fenomena langka atau penting.\n",
        "\n",
        "\n",
        "##### Penanganan Outliers\n",
        "1. Menghapus Outliers: Jika outliers disebabkan oleh kesalahan pengukuran atau data yang tidak relevan, mereka dapat dihapus dari dataset.\n",
        "2. Transformasi Data: Menggunakan teknik seperti log transformation untuk mengurangi dampak outliers.\n",
        "3. Model yang Tahan Outliers: Menggunakan model machine learning yang lebih tahan terhadap outliers, seperti tree-based methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNfEwF1wBwZN"
      },
      "outputs": [],
      "source": [
        "# Langkah 1: Membuat DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eve', np.nan],\n",
        "    'Age': [25, 30, 35, 25, np.nan, 50],\n",
        "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Miami', 'Los Angeles']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Langkah 2: Identifikasi Outliers\n",
        "# Outliers dapat diidentifikasi menggunakan berbagai metode, salah satu yang umum adalah menggunakan Z-score atau IQR (Interquartile Range).\n",
        "# Di sini, kita akan menggunakan IQR untuk mendeteksi outliers.\n",
        "# Menghitung IQR\n",
        "Q1 = df['Age'].quantile(0.25)\n",
        "Q3 = df['Age'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Menentukan batas bawah dan atas untuk outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Menandai outliers\n",
        "outliers = df[(df['Age'] < lower_bound) | (df['Age'] > upper_bound)]\n",
        "print(\"Outliers:\\n\", outliers)\n",
        "\n",
        "\n",
        "# Langkah 3: Menangani Outliers\n",
        "# # Menghapus outliers\n",
        "df_no_outliers = df[(df['Age'] >= lower_bound) & (df['Age'] <= upper_bound)]\n",
        "\n",
        "# Mengganti outliers dengan nilai median\n",
        "median_age = df['Age'].median()\n",
        "df['Age'] = np.where((df['Age'] < lower_bound) | (df['Age'] > upper_bound), median_age, df['Age'])\n",
        "\n",
        "# Mengisi missing values dengan median\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Name'].fillna('Unknown', inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brw3rc88BwZO"
      },
      "source": [
        "### D. Mengubah Data Kategorikal ke Numerik\n",
        "Mengubah data kategorikal ke numerik dalam analisis data dan pemodelan machine learning sangat penting karena sebagian besar algoritma machine learning membutuhkan input numerik untuk melakukan perhitungan. Ada beberapa alasan utama mengapa transformasi ini diperlukan:\n",
        "1. Kompatibilitas dengan Algoritma: Banyak algoritma machine learning, seperti regresi linier, regresi logistik, dan algoritma berbasis jarak seperti KNN, tidak dapat beroperasi dengan data kategorikal. Mereka membutuhkan data numerik untuk menghitung jarak, koefisien, dan parameter lainnya.\n",
        "2. Kinerja dan Akurasi Model: Mengubah data kategorikal menjadi format numerik memungkinkan model untuk lebih efektif mengenali pola dalam data. Representasi numerik dapat membantu dalam memanfaatkan informasi yang terkandung dalam variabel kategorikal, yang dapat meningkatkan kinerja dan akurasi model.\n",
        "3. Skalabilitas dan Efisiensi: Algoritma yang bekerja dengan data numerik sering kali lebih efisien dan skalabel dibandingkan dengan yang harus bekerja dengan data kategorikal. Data numerik memungkinkan operasi matematika dan statistik yang lebih cepat dan lebih mudah diterapkan.\n",
        "4. Persyaratan Prapemrosesan: Beberapa langkah prapemrosesan data, seperti normalisasi atau standardisasi, memerlukan data dalam bentuk numerik. Ini penting untuk memastikan bahwa semua fitur dalam dataset berada pada skala yang sama, yang bisa mempengaruhi hasil dari algoritma ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-pKXpuABwZO"
      },
      "outputs": [],
      "source": [
        "# Contoh: misalnya, kita memiliki dataset dengan kolom \"City\" yang berisi data kategorikal:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Miami']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# One-Hot Encoding\n",
        "df_one_hot = pd.get_dummies(df, columns=['City'])\n",
        "\n",
        "print(df_one_hot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "fNTmW13-BwZO"
      },
      "source": [
        "### E. Menangani Data Tidak Valid\n",
        "Menangani data tidak valid adalah proses penting untuk memastikan integritas dan kualitas dataset. Data tidak valid dapat mengganggu analisis dan menghasilkan model yang tidak akurat. Berikut adalah langkah-langkah yang bisa diambil untuk menangani data tidak valid:\n",
        "\n",
        "##### Identifikasi Data Tidak Valid\n",
        "1. Pemeriksaan Kesalahan: Identifikasi kesalahan ketik atau entri yang tidak sesuai format yang diharapkan.\n",
        "2. Pemeriksaan Batas Nilai: Pastikan nilai numerik berada dalam rentang yang wajar.\n",
        "3. Pemeriksaan Konsistensi: Pastikan konsistensi antar kolom, misalnya tanggal mulai tidak boleh setelah tanggal berakhir.\n",
        "4. Deteksi Anomali: Gunakan metode statistik atau algoritma untuk mendeteksi anomali dalam data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPHHYr1gBwZO"
      },
      "source": [
        "##### Penanganan Data Tidak Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXXaL2F2BwZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eve', np.nan],\n",
        "    'Age': [25, 30, 35, 25, np.nan, 50],\n",
        "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Miami', 'Los Angeles']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Menghapus Data Tidak Valid: Jika jumlah data tidak valid kecil dan tidak signifikan, menghapusnya bisa menjadi solusi yang cepat dan mudah.\n",
        "df = df.dropna(subset=['Age', 'Name'])  # Menghapus baris dengan nilai 'Age' atau 'Name' yang tidak valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvBQwaxxBwZP"
      },
      "outputs": [],
      "source": [
        "# Mengganti dengan Nilai Lain:\n",
        "\n",
        "# Mengisi dengan Rata-rata/Median: Mengisi nilai yang hilang atau tidak valid dengan rata-rata atau median.\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# Mengisi dengan Nilai Default: Mengisi dengan nilai default yang logis atau sesuai konteks.\n",
        "df['Name'] = df['Name'].fillna('Unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCYtPmTrBwZP"
      },
      "outputs": [],
      "source": [
        "# Transformasi Data: Mengubah data tidak valid menjadi format yang sesuai.\n",
        "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')  # Mengubah nilai 'Age' yang tidak valid menjadi NaN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfOS7koXBwZP"
      },
      "outputs": [],
      "source": [
        "# Pembersihan dengan Logika Bisnis: Menggunakan aturan bisnis untuk memperbaiki data. Misalnya, jika usia tidak masuk akal (misalnya lebih dari 120 tahun), maka ubah atau hapus.\n",
        "df = df[(df['Age'] >= 0) & (df['Age'] <= 120)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eA3xCk4BwZQ"
      },
      "source": [
        "##### Contoh Menangani Data Tidak Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EoHYQiwBwZQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eve', np.nan],\n",
        "    'Age': [25, 30, 35, 25, np.nan, 150],  # 150 dianggap sebagai nilai tidak valid\n",
        "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Miami', 'Los Angeles']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Identifikasi nilai tidak valid\n",
        "print(\"Data Awal:\")\n",
        "print(df)\n",
        "\n",
        "# Mengubah nilai 'Age' yang tidak valid menjadi NaN\n",
        "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
        "\n",
        "# Mengisi nilai 'Age' yang hilang atau tidak valid dengan median\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# Mengisi nilai 'Name' yang hilang dengan 'Unknown'\n",
        "df['Name'] = df['Name'].fillna('Unknown')\n",
        "\n",
        "print(\"\\nData Setelah Menangani Nilai Tidak Valid:\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2I7gkLfBwZQ"
      },
      "source": [
        "Dalam contoh ini, nilai usia yang tidak valid (150) diubah menjadi nilai median, dan nama yang hilang diisi dengan 'Unknown'. Langkah-langkah ini membantu memastikan bahwa data yang digunakan untuk analisis dan pemodelan lebih bersih dan dapat diandalkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ptuOciBwZQ"
      },
      "source": [
        "## 2. Data Normalization\n",
        "Data normalization adalah proses mengubah skala fitur dalam dataset sehingga mereka berada dalam rentang yang sama atau memiliki distribusi yang serupa. Normalisasi data penting untuk meningkatkan kinerja algoritma machine learning, terutama yang sensitif terhadap skala data seperti k-NN, regresi linier, dan neural networks. Mengapa Data Normalization Penting?\n",
        "1. Konsistensi Skala: Algoritma berbasis jarak seperti k-NN dan K-Means clustering sangat bergantung pada jarak antar titik data. Normalisasi memastikan bahwa semua fitur berkontribusi secara proporsional terhadap jarak tersebut.\n",
        "2. Stabilitas Numerik: Model seperti regresi linier dan neural networks dapat mengalami masalah stabilitas numerik jika fitur memiliki skala yang sangat berbeda. Normalisasi membantu menghindari masalah ini dengan menjaga nilai fitur dalam rentang yang seragam.\n",
        "3. Kecepatan Konvergensi: Normalisasi dapat meningkatkan kecepatan konvergensi algoritma pembelajaran seperti gradient descent dengan menghindari jalan yang curam di sepanjang satu dimensi dan datar di dimensi lainnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bvjZR9sBwZQ"
      },
      "source": [
        "#### Metode Normalisasi Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCGPdgCiBwZQ"
      },
      "source": [
        "##### a. Min-Max Scaling: Mengubah data sehingga berada dalam rentang tertentu, biasanya [0, 1].\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2XORryYBwZR"
      },
      "source": [
        "##### b. Z-Score Normalization (Standardization): Mengubah data sehingga memiliki rata-rata 0 dan standar deviasi 1.\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "di mana μμ adalah rata-rata dan σσ adalah standar deviasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO3IOADqBwZR"
      },
      "source": [
        "##### c. Robust Scaler: Menggunakan median dan IQR (Interquartile Range) untuk mengurangi pengaruh outliers.\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmJldDizBwZR"
      },
      "source": [
        "##### Contoh Normalisasi dengan Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Viu_bWKPBwZR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Contoh dataset\n",
        "data = {\n",
        "    'Age': [25, 30, 35, 40, 45],\n",
        "    'Salary': [50000, 60000, 70000, 80000, 90000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Min-Max Scaling\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Z-Score Normalization\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"Data Asli:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nMin-Max Scaled Data:\")\n",
        "print(df_min_max_scaled)\n",
        "\n",
        "print(\"\\nStandardized Data:\")\n",
        "print(df_standard_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNqPnh0FBwZS"
      },
      "source": [
        "Dalam contoh ini, kita melihat bagaimana fitur Age dan Salary diubah menggunakan Min-Max Scaling dan Z-Score Normalization. Setelah normalisasi, data berada dalam rentang yang lebih seragam, yang membantu meningkatkan kinerja model machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9qDeRwQBwZS"
      },
      "source": [
        "## 3. Validasi Model\n",
        "Validasi model adalah proses evaluasi model machine learning untuk memastikan bahwa model tersebut bekerja dengan baik dan dapat diandalkan ketika diaplikasikan pada data baru. Tujuan utama validasi model adalah untuk mengukur kinerja model, mencegah overfitting, dan memastikan generalisasi model. Berikut adalah penjelasan lebih lanjut tentang tujuan dan teknik validasi model. Tujuan Validasi Model\n",
        "1. Mengukur Kinerja Model: Validasi model membantu mengukur seberapa baik model bekerja dalam hal akurasi, presisi, recall, F1-score, dan metrik lainnya.\n",
        "2. Mencegah Overfitting: Overfitting terjadi ketika model terlalu menyesuaikan diri dengan data pelatihan dan gagal menggeneralisasi pada data baru. Validasi model membantu mengidentifikasi dan mengurangi overfitting.\n",
        "3. Memastikan Generalisasi: Validasi memastikan bahwa model dapat bekerja dengan baik pada data yang tidak terlihat sebelumnya, yang sangat penting untuk aplikasi dunia nyata.\n",
        "4. Pemilihan Model: Membantu dalam pemilihan model terbaik dari beberapa model atau konfigurasi berdasarkan kinerja pada set validasi.\n",
        "5. Hyperparameter Tuning: Membantu dalam menyetel hyperparameter model untuk mendapatkan kinerja terbaik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99WfkeLnBwZS"
      },
      "source": [
        "#### Teknik Validasi Model\n",
        "1. Holdout Validation: Dataset dibagi menjadi dua subset: satu untuk pelatihan (training set) dan satu lagi untuk pengujian (test set). Contoh: 70% data untuk pelatihan dan 30% data untuk pengujian. Kelemahan: Hasil bisa sangat bergantung pada cara data dibagi.\n",
        "2. K-Fold Cross-Validation: Dataset dibagi menjadi k subset (folds). Model dilatih k kali, setiap kali menggunakan k-1 folds untuk pelatihan dan 1 fold untuk pengujian. Contoh: 5-Fold Cross-Validation. Keuntungan: Lebih akurat dan stabil karena menggunakan seluruh dataset untuk pelatihan dan pengujian.\n",
        "3. Stratified K-Fold Cross-Validation: Mirip dengan K-Fold, tetapi memastikan bahwa setiap fold memiliki proporsi yang sama dari kelas target, menjaga distribusi kelas yang konsisten di seluruh fold. Keuntungan: Berguna untuk dataset yang tidak seimbang.\n",
        "4. Leave-One-Out Cross-Validation (LOOCV): Setiap contoh data digunakan sekali sebagai set pengujian sementara sisanya digunakan sebagai set pelatihan. Keuntungan: Menggunakan semua data yang tersedia untuk pelatihan. Kelemahan: Sangat memakan waktu dan komputasi untuk dataset besar.\n",
        "5. Time Series Cross-Validation: Digunakan untuk data urutan waktu. Data dibagi berdasarkan urutan waktu, memastikan bahwa model hanya dilatih pada data dari masa lalu dan diuji pada data dari masa depan. Keuntungan: Menghormati urutan temporal dan cocok untuk data deret waktu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81IdMDJpBwZT"
      },
      "source": [
        "##### Contoh Implementasi K-Fold Cross-Validation di Python\n",
        "Berikut adalah contoh bagaimana mengimplementasikan K-Fold Cross-Validation menggunakan scikit-learn di Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cHo2dfrBwZT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Contoh dataset\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "y = np.array([2, 3, 4, 5, 6])\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5)\n",
        "scores = cross_val_score(model, X, y, cv=kf)\n",
        "\n",
        "print(\"K-Fold Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHN_yZ_FBwZT"
      },
      "source": [
        "Outputnya adalah skor kinerja model untuk setiap fold dan rata-rata skornya, yang memberikan gambaran umum tentang kinerja model Validasi model adalah langkah kritis dalam machine learning yang membantu memastikan bahwa model yang dikembangkan adalah akurat, andal, dan mampu menangani data yang tidak terlihat dengan baik. Dengan menggunakan teknik validasi yang tepat, kita dapat memilih model terbaik dan mengoptimalkan kinerjanya untuk aplikasi dunia nyata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgiv_4ZNBwZT"
      },
      "source": [
        "## 4. K-Fold Cross-Validation\n",
        "K-Fold Cross-Validation adalah salah satu teknik validasi model yang umum digunakan dalam machine learning untuk mengevaluasi kinerja model secara lebih akurat. Teknik ini membagi dataset menjadi k subset (folds) yang sama besar, di mana setiap fold digunakan sebagai set pengujian satu kali sementara k-1 folds lainnya digunakan sebagai set pelatihan. Berikut ini adalah langkah-langkah dan manfaat utama dari K-Fold Cross-Validation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hINEkzqvBwZT"
      },
      "source": [
        "##### Langkah-langkah K-Fold Cross-Validation\n",
        "1. Pembagian Dataset: Dataset dibagi menjadi k subset atau folds yang sama besar.\n",
        "2. Iterasi Training dan Testing: Model dilatih k kali menggunakan kombinasi berbeda dari k-1 folds untuk pelatihan dan 1 fold untuk pengujian setiap kali.\n",
        "3. Evaluasi Kinerja: Untuk setiap iterasi, kinerja model dievaluasi menggunakan metrik evaluasi yang relevan (misalnya, akurasi, presisi, recall).\n",
        "4. Perhitungan Rata-rata: Hasil evaluasi dari setiap iterasi diambil rata-ratanya untuk memberikan estimasi akhir dari kinerja model.\n",
        "\n",
        "##### Manfaat K-Fold Cross-Validation\n",
        "1. Menggunakan Data Secara Efisien: Memanfaatkan seluruh dataset untuk pelatihan dan pengujian, menghasilkan estimasi kinerja model yang lebih stabil dan akurat.\n",
        "2. Pencegahan Overfitting: Dengan menggunakan k-1 folds untuk pelatihan pada setiap iterasi, K-Fold Cross-Validation membantu mengurangi risiko overfitting model terhadap data pelatihan tertentu.\n",
        "3. Penggunaan yang Luas: Teknik ini dapat diterapkan pada berbagai jenis model dan dataset, termasuk data yang tidak seimbang atau data deret waktu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rogy5qgsBwZU"
      },
      "outputs": [],
      "source": [
        "# Implementasi K-Fold Cross-Validation di Python\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Misalnya, menggunakan 5 folds dengan shuffle dan seed random 42\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "print(\"K-Fold Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Score:\", scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny8E8fJ2BwZU"
      },
      "source": [
        "Dalam contoh di atas, dataset Iris dibagi menjadi 5 folds dengan shuffle dan seed random yang ditentukan. Model Regresi Logistik dievaluasi menggunakan metrik akurasi, dan hasil dari setiap fold dievaluasi dan dihitung rata-ratanya. K-Fold Cross-Validation adalah alat yang penting dalam evaluasi model machine learning untuk memastikan bahwa model yang dikembangkan dapat diandalkan dan dapat digeneralisasikan dengan baik pada data yang tidak terlihat sebelumnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuydhDpYBwZU"
      },
      "source": [
        "## 5. Confussion Matrix\n",
        "Confusion Matrix (matriks kebingungan) adalah tabel yang digunakan untuk mengevaluasi kinerja suatu model klasifikasi pada sebuah dataset yang sudah diketahui labelnya. Matriks ini menggambarkan jumlah hasil prediksi yang benar dan yang salah dalam empat kategori berbeda. Berikut ini adalah komponen confusion matrix:\n",
        "1. True Positive (TP): Kasus di mana model memprediksi positif (1) dan kenyataannya positif (1).\n",
        "2. True Negative (TN): Kasus di mana model memprediksi negatif (0) dan kenyataannya negatif (0).\n",
        "3. False Positive (FP) (Type I Error): Kasus di mana model memprediksi positif (1) tetapi kenyataannya negatif (0). Juga dikenal sebagai kesalahan tipe I.\n",
        "4. False Negative (FN) (Type II Error): Kasus di mana model memprediksi negatif (0) tetapi kenyataannya positif (1). Juga dikenal sebagai kesalahan tipe II."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrAx2tppBwZU"
      },
      "source": [
        "#### Interpretasi Confusion Matrix\n",
        "Matriks kebingungan memberikan gambaran yang lebih komprehensif tentang kinerja model klasifikasi daripada metrik evaluasi tunggal seperti akurasi. Dengan menggunakan komponen-komponen tersebut, beberapa metrik evaluasi lain dapat dihitung, seperti:\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD3sIYBABwZU"
      },
      "source": [
        "##### Contoh Confusion Matrix\n",
        "\n",
        "Misalkan kita memiliki sebuah confusion matrix sebagai berikut:\n",
        "\n",
        "```lua\n",
        "\n",
        "            Predicted\n",
        "         |  0   |  1  |\n",
        "Actual   |------|-----|\n",
        "   0     |  50  |  10 |\n",
        "   1     |  5   |  35 |\n",
        "\n",
        "\n",
        "Dari matriks ini:\n",
        "\n",
        "    True Positive (TP) = 50\n",
        "    True Negative (TN) = 35\n",
        "    False Positive (FP) = 10\n",
        "    False Negative (FN) = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa0gubOrBwZU"
      },
      "source": [
        "Dengan informasi ini, kita bisa menghitung akurasi, presisi, recall, dan F1-score untuk mengevaluasi performa model klasifikasi tersebut. Confusion matrix adalah alat yang sangat berguna untuk memahami seberapa baik model klasifikasi kita bekerja pada dataset tertentu, terutama dalam konteks klasifikasi yang tidak seimbang atau ketika perlu menilai dampak dari kesalahan prediksi tertentu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOgJrWvYBwZV"
      },
      "source": [
        "## 6. Bootstrap Sampling\n",
        "Bootstrap sampling adalah teknik resampling yang digunakan dalam statistik untuk mengevaluasi keandalan estimasi dari sebuah sampel data. Teknik ini memungkinkan kita untuk membuat estimasi dari sebaran (distribution) suatu statistik, seperti mean, median, atau deviasi standar, bahkan jika distribusi dari populasi tidak diketahui.\n",
        "\n",
        "##### Konsep Dasar Bootstrap Sampling\n",
        "1. Resampling: Bootstrap sampling melibatkan pengambilan sampel dari dataset yang ada dengan mengembalikan (dengan penggantian) observasi yang sudah diambil. Ini berarti setiap observasi dalam dataset asli memiliki kesempatan untuk muncul di dalam sampel bootstrap lebih dari satu kali atau bahkan tidak muncul sama sekali.\n",
        "2. Estimasi Sebaran: Dengan membuat banyak sampel bootstrap (biasanya ribuan sampai jutaan), kita dapat membangun distribusi dari statistik yang ingin diestimasi. Misalnya, kita dapat menghitung mean dari setiap sampel bootstrap, dan distribusi dari mean tersebut akan memberi kita perkiraan tentang sebaran mean dari populasi.\n",
        "3. Keuntungan: Bootstrap sampling mengatasi masalah ketidakpastian tentang distribusi dari populasi, karena kita tidak harus bergantung pada asumsi tertentu tentang distribusi. Teknik ini juga memberikan interval kepercayaan (confidence intervals) yang lebih akurat.\n",
        "\n",
        "##### Langkah-langkah Implementasi Bootstrap Sampling\n",
        "1. Ambil Sampel: Ambil sampel dari dataset asli sebanyak n kali, dengan penggantian.\n",
        "2. Hitung Statistik: Hitung statistik dari setiap sampel bootstrap, seperti mean, median, deviasi standar, atau persentil.\n",
        "3. Bangun Distribusi: Dengan menggunakan hasil statistik dari banyak sampel bootstrap, bangun distribusi dari statistik tersebut.\n",
        "4. Interval Kepercayaan: Dengan distribusi yang telah dibangun, hitung interval kepercayaan untuk estimasi statistik tertentu, misalnya 95% confidence interval.\n",
        "\n",
        "##### Contoh Bootstrap Sampling\n",
        "Misalkan kita memiliki dataset berikut yang berisi nilai-nilai tinggi dari suatu populasi:\n",
        "\n",
        "Data: [10, 15, 8, 12, 14, 20, 18, 16, 11, 13]\n",
        "\n",
        "Bootstrap sampling adalah teknik resampling yang digunakan dalam statistik untuk mengevaluasi keandalan estimasi dari sebuah sampel data. Teknik ini memungkinkan kita untuk membuat estimasi dari sebaran (distribution) suatu statistik, seperti mean, median, atau deviasi standar, bahkan jika distribusi dari populasi tidak diketahui.\n",
        "\n",
        "##### Konsep Dasar Bootstrap Sampling\n",
        "1. Resampling: Bootstrap sampling melibatkan pengambilan sampel dari dataset yang ada dengan mengembalikan (dengan penggantian) observasi yang sudah diambil. Ini berarti setiap observasi dalam dataset asli memiliki kesempatan untuk muncul di dalam sampel bootstrap lebih dari satu kali atau bahkan tidak muncul sama sekali.\n",
        "2. Estimasi Sebaran: Dengan membuat banyak sampel bootstrap (biasanya ribuan sampai jutaan), kita dapat membangun distribusi dari statistik yang ingin diestimasi. Misalnya, kita dapat menghitung mean dari setiap sampel bootstrap, dan distribusi dari mean tersebut akan memberi kita perkiraan tentang sebaran mean dari populasi.\n",
        "3. Keuntungan: Bootstrap sampling mengatasi masalah ketidakpastian tentang distribusi dari populasi, karena kita tidak harus bergantung pada asumsi tertentu tentang distribusi. Teknik ini juga memberikan interval kepercayaan (confidence intervals) yang lebih akurat.\n",
        "\n",
        "##### Langkah-langkah Implementasi Bootstrap Sampling\n",
        "1. Ambil Sampel: Ambil sampel dari dataset asli sebanyak n kali, dengan penggantian.\n",
        "2. Hitung Statistik: Hitung statistik dari setiap sampel bootstrap, seperti mean, median, deviasi standar, atau persentil.\n",
        "3. Bangun Distribusi: Dengan menggunakan hasil statistik dari banyak sampel bootstrap, bangun distribusi dari statistik tersebut.\n",
        "4. Interval Kepercayaan: Dengan distribusi yang telah dibangun, hitung interval kepercayaan untuk estimasi statistik tertentu, misalnya 95% confidence interval.\n",
        "\n",
        "##### Contoh Bootstrap Sampling\n",
        "\n",
        "Misalkan kita memiliki dataset berikut yang berisi nilai-nilai tinggi dari suatu populasi:\n",
        "\n",
        "Data: [10, 15, 8, 12, 14, 20, 18, 16, 11, 13]\n",
        "\n",
        "Kita ingin mengevaluasi rata-rata (mean) dari populasi ini menggunakan bootstrap sampling:\n",
        "1. Ambil Sampel: Ambil n sampel dengan penggantian dari dataset asli. Misalnya, kita ambil 1000 sampel bootstrap.\n",
        "2. Hitung Mean: Hitung mean dari setiap sampel bootstrap.\n",
        "3. Bangun Distribusi: Dengan menggunakan mean dari sampel bootstrap, kita dapat membangun distribusi mean dari populasi.\n",
        "4. Interval Kepercayaan: Hitung 95% confidence interval dari distribusi mean yang dibangun."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZD-4AOTBwZV"
      },
      "source": [
        "Berikut adalah contoh implementasi sederhana menggunakan Python untuk menghitung mean dan confidence interval dengan bootstrap sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHhC9e6vBwZV",
        "outputId": "ebd245f4-fcc4-487c-ce4b-64855a8d5636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean: 13.7\n",
            "95% Confidence Interval: 11.6975 - 15.9\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "data = np.array([10, 15, 8, 12, 14, 20, 18, 16, 11, 13])\n",
        "\n",
        "# Bootstrap sampling\n",
        "n_samples = 1000\n",
        "bootstrap_means = np.zeros(n_samples)\n",
        "\n",
        "for i in range(n_samples):\n",
        "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
        "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
        "\n",
        "# Confidence interval (95%)\n",
        "ci_lower = np.percentile(bootstrap_means, 2.5)\n",
        "ci_upper = np.percentile(bootstrap_means, 97.5)\n",
        "\n",
        "print(\"Mean:\", np.mean(data))\n",
        "print(\"95% Confidence Interval:\", ci_lower, \"-\", ci_upper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZuxUVWcBwZW"
      },
      "source": [
        "Dalam contoh ini, kita mengambil 1000 sampel bootstrap dari data, menghitung mean dari setiap sampel, dan kemudian menghitung 95% confidence interval dari distribusi mean yang dibangun. Bootstrap sampling adalah alat yang powerful untuk memperoleh estimasi yang stabil dan distribusi statistik dari data yang ada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8EI2nJkBwZW"
      },
      "source": [
        "## 7. Evaluasi Kinerja Model\n",
        "Evaluasi kinerja model adalah proses penting dalam machine learning untuk menilai seberapa baik model dapat melakukan prediksi terhadap data yang belum pernah dilihat sebelumnya. Tujuannya adalah untuk memastikan bahwa model dapat digeneralisasikan dengan baik dan bekerja dengan akurat pada situasi dunia nyata. Berikut ini adalah langkah-langkah umum dalam evaluasi kinerja model:\n",
        "\n",
        "##### Langkah-langkah Evaluasi Kinerja Model\n",
        "1. Pembagian Dataset: Bagi dataset menjadi set pelatihan (training set) dan set pengujian (test set) secara acak. Data pelatihan digunakan untuk melatih model, sementara data pengujian digunakan untuk mengevaluasi kinerja model.\n",
        "2. Pemilihan Metrik Evaluasi: Pilih metrik evaluasi yang sesuai tergantung pada jenis masalah yang sedang diselesaikan. Beberapa metrik umum termasuk:\n",
        "   - Classification: Akurasi, presisi, recall, F1-score, ROC-AUC.\n",
        "   - Regression: Mean Squared Error (MSE), R-squared, Mean Absolute Error (MAE).\n",
        "3. Pembuatan Model: Pilih algoritma model yang sesuai dengan jenis masalah (klasifikasi, regresi, clustering, dll.) dan latih model menggunakan data pelatihan.\n",
        "4. Evaluasi pada Data Pengujian: Evaluasi model pada data pengujian menggunakan metrik yang telah dipilih. Ini memberikan gambaran tentang seberapa baik model akan berperforma pada data baru yang belum pernah dilihat sebelumnya.\n",
        "5. Validasi Silang (Cross-Validation): Gunakan teknik validasi silang seperti K-Fold Cross-Validation untuk mendapatkan estimasi yang lebih baik tentang kinerja model. Ini membantu dalam mengurangi variabilitas hasil evaluasi yang mungkin disebabkan oleh pembagian acak data.\n",
        "6. Analisis Confusion Matrix (untuk klasifikasi): Jika model adalah model klasifikasi, analisis confusion matrix membantu dalam memahami seberapa baik model dapat mengklasifikasikan instance dari setiap kelas.\n",
        "7. Penyetelan Hyperparameter: Lakukan penyetelan hyperparameter untuk meningkatkan kinerja model. Gunakan teknik seperti Grid Search atau Random Search untuk menemukan kombinasi hyperparameter terbaik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR44-hGeBwZW"
      },
      "source": [
        "##### Contoh Implementasi Evaluasi Kinerja Model di Python\n",
        "Berikut adalah contoh sederhana menggunakan Python untuk evaluasi kinerja model klasifikasi menggunakan metrik akurasi dan confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk1DE1AdBwZW",
        "outputId": "49f10542-0734-47e8-d6d2-fd2424c94ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Bagi dataset menjadi data pelatihan dan data pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inisialisasi dan latih model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi dengan data pengujian\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluasi kinerja model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "19q95LD1BwZX",
        "outputId": "5e01bd99-c26a-4378-87e8-f30bd5a04c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[170  29]\n",
            " [ 37 164]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.82      0.85      0.84       199\n",
            "         pos       0.85      0.82      0.83       201\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.84      0.83       400\n",
            "weighted avg       0.84      0.83      0.83       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contoh Confussion Matrix di NLP\n",
        "# Impor library yang diperlukan\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Unduh dataset sentimen ulasan film dari NLTK\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Ambil ulasan dan label dari dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Pisahkan teks ulasan dan label\n",
        "texts = [' '.join(document) for document, category in documents]\n",
        "labels = [category for document, category in documents]\n",
        "\n",
        "# Ubah teks menjadi vektor fitur TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Bagi dataset menjadi data pelatihan dan data pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inisialisasi dan latih model klasifikasi (misalnya, Linear SVM)\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi kelas pada data pengujian\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluasi model menggunakan confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Evaluasi model menggunakan classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}